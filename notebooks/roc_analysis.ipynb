{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis for CIFAR-10 CNN Experiments\n",
    "\n",
    "This notebook computes ROC curves and AUC scores for CIFAR-10 CNN classification experiments.\n",
    "It retrieves prediction probability files as assets from the DerivaML catalog.\n",
    "\n",
    "**Input:** List of asset RIDs for probability files (configured via `assets` parameter)\n",
    "\n",
    "**Output:** ROC curves comparing model performance across experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from deriva_ml import DerivaML\n",
    "from deriva_ml.execution import Execution, ExecutionConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Configure asset RIDs to analyze. These should be `prediction_probabilities.csv` files\n",
    "from completed CIFAR-10 CNN executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell - injected by papermill or deriva-ml-run-notebook\n",
    "host: str = \"localhost\"\n",
    "catalog: str = \"45\"\n",
    "# Asset RIDs for prediction probability files\n",
    "assets: list[str] = [\"3JSJ\", \"3KVC\"]  # Default: multirun comparison probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Create Execution Context and Download Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DerivaML connection\n",
    "ml = DerivaML(hostname=host, catalog_id=catalog)\n",
    "print(f\"Connected to {ml.host_name}, catalog {ml.catalog_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create execution context with assets - this downloads the probability files\n",
    "workflow = ml.create_workflow(\n",
    "    name=\"ROC Analysis\",\n",
    "    workflow_type=\"ROC Analysis Notebook\",\n",
    "    description=\"Compute ROC curves and AUC scores for CIFAR-10 CNN experiments\",\n",
    ")\n",
    "exec_config = ExecutionConfiguration(\n",
    "    workflow=workflow,\n",
    "    assets=assets,\n",
    "    description=f\"ROC analysis of {len(assets)} experiments\",\n",
    ")\n",
    "execution = Execution(configuration=exec_config, ml_object=ml)\n",
    "print(f\"Created execution {execution.execution_rid}\")\n",
    "print(f\"Downloaded assets: {list(execution.asset_paths.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probability files from downloaded assets\n",
    "experiments = []\n",
    "for asset_table, asset_list in execution.asset_paths.items():\n",
    "    for asset_path in asset_list:\n",
    "        if asset_path.file_name.name == \"prediction_probabilities.csv\":\n",
    "            df = pd.read_csv(asset_path.file_name)\n",
    "            experiments.append({\n",
    "                'asset_rid': asset_path.asset_rid,\n",
    "                'file_name': asset_path.file_name.name,\n",
    "                'data': df\n",
    "            })\n",
    "            print(f\"Loaded {len(df)} predictions from asset {asset_path.asset_rid}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Get Ground Truth Labels\n",
    "\n",
    "Retrieve ground truth labels from the Image_Classification feature table.\n",
    "Ground truth labels are those without confidence scores (manually assigned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels from the feature table\n",
    "all_feature_values = list(ml.list_feature_values(\"Image\", \"Image_Classification\"))\n",
    "feature_df = pd.DataFrame(all_feature_values)\n",
    "\n",
    "# Ground truth labels have no confidence score (manually labeled)\n",
    "# Group by execution to identify which has ground truth\n",
    "exec_summary = feature_df.groupby('Execution').agg({\n",
    "    'Image': 'count',\n",
    "    'Confidence': lambda x: x.notna().sum()\n",
    "}).rename(columns={'Image': 'num_images', 'Confidence': 'with_confidence'})\n",
    "\n",
    "# Find execution with no confidence scores (ground truth)\n",
    "gt_mask = exec_summary['with_confidence'] == 0\n",
    "if gt_mask.any():\n",
    "    gt_execution = exec_summary[gt_mask].index[0]\n",
    "else:\n",
    "    gt_execution = exec_summary['num_images'].idxmax()\n",
    "    \n",
    "print(f\"Ground truth execution: {gt_execution}\")\n",
    "print(f\"Total ground truth labels: {exec_summary.loc[gt_execution, 'num_images']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth as lookup dictionary\n",
    "ground_truth = feature_df[feature_df['Execution'] == gt_execution][['Image', 'Image_Class']]\n",
    "gt_lookup = dict(zip(ground_truth['Image'], ground_truth['Image_Class']))\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted(ground_truth['Image_Class'].unique())\n",
    "n_classes = len(class_names)\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Merge Predictions with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ground truth to each experiment's predictions\n",
    "for exp in experiments:\n",
    "    df = exp['data'].copy()\n",
    "    \n",
    "    # Debug: show sample RIDs from predictions vs ground truth\n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Prediction Image_RIDs (first 5): {df['Image_RID'].head().tolist()}\")\n",
    "    print(f\"  Ground truth Image keys (first 5): {list(gt_lookup.keys())[:5]}\")\n",
    "    \n",
    "    df['True_Class'] = df['Image_RID'].map(gt_lookup)\n",
    "    # Keep only images with ground truth\n",
    "    matched = df['True_Class'].notna().sum()\n",
    "    print(f\"  Matched: {matched} / {len(df)}\")\n",
    "    \n",
    "    df = df.dropna(subset=['True_Class'])\n",
    "    exp['data'] = df\n",
    "    exp['n_samples'] = len(df)\n",
    "    if len(df) > 0:\n",
    "        exp['accuracy'] = (df['Predicted_Class'] == df['True_Class']).mean() * 100\n",
    "        print(f\"  Accuracy: {exp['accuracy']:.1f}%\")\n",
    "    else:\n",
    "        exp['accuracy'] = float('nan')\n",
    "        print(\"  No matching samples found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Compute ROC Curves\n",
    "\n",
    "For multi-class classification, compute ROC curves using one-vs-rest approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_metrics(df: pd.DataFrame, class_names: list[str]) -> dict:\n",
    "    \"\"\"Compute ROC curves and AUC scores for multi-class predictions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with True_Class and prob_* columns\n",
    "        class_names: Ordered list of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dict with fpr, tpr, roc_auc for each class and micro/macro averages\n",
    "    \"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "    \n",
    "    # Convert labels to indices\n",
    "    y_true_idx = df['True_Class'].map(class_to_idx).values\n",
    "    y_true_bin = label_binarize(y_true_idx, classes=range(n_classes))\n",
    "    \n",
    "    # Get probability matrix\n",
    "    prob_cols = [f\"prob_{c}\" for c in class_names]\n",
    "    y_score = df[prob_cols].values\n",
    "    \n",
    "    # Compute per-class ROC\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        fpr[name], tpr[name], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc[name] = auc(fpr[name], tpr[name])\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "    \n",
    "    # Macro-average\n",
    "    roc_auc['macro'] = np.mean([roc_auc[c] for c in class_names])\n",
    "    \n",
    "    return {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC metrics for each experiment\n",
    "for exp in experiments:\n",
    "    metrics = compute_roc_metrics(exp['data'], class_names)\n",
    "    exp.update(metrics)\n",
    "    \n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"  Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"  Macro-AUC: {exp['roc_auc']['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC comparison table\n",
    "if experiments:\n",
    "    auc_data = []\n",
    "    for exp in experiments:\n",
    "        row = {'Asset': exp['asset_rid']}\n",
    "        for c in class_names:\n",
    "            row[c] = exp['roc_auc'][c]\n",
    "        row['Micro'] = exp['roc_auc']['micro']\n",
    "        row['Macro'] = exp['roc_auc']['macro']\n",
    "        auc_data.append(row)\n",
    "\n",
    "    auc_df = pd.DataFrame(auc_data).set_index('Asset')\n",
    "    print(\"\\nPer-class AUC scores:\")\n",
    "    display(auc_df.round(4))\n",
    "else:\n",
    "    print(\"No experiments loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(exp: dict, class_names: list[str]):\n",
    "    \"\"\"Plot ROC curves for all classes in an experiment.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    fpr, tpr, roc_auc = exp['fpr'], exp['tpr'], exp['roc_auc']\n",
    "    \n",
    "    # Micro-average\n",
    "    ax.plot(fpr['micro'], tpr['micro'], \n",
    "            label=f\"Micro-avg (AUC={roc_auc['micro']:.3f})\",\n",
    "            color='deeppink', linestyle=':', linewidth=3)\n",
    "    \n",
    "    # Per-class\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "    for i, name in enumerate(class_names):\n",
    "        ax.plot(fpr[name], tpr[name], color=colors[i],\n",
    "                label=f\"{name} (AUC={roc_auc[name]:.3f})\")\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f\"ROC Curves - Asset {exp['asset_rid']} (Acc: {exp['accuracy']:.1f}%)\")\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each experiment\n",
    "for exp in experiments:\n",
    "    fig = plot_roc_curves(exp, class_names)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Experiment Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare micro-average ROC curves across experiments\n",
    "if len(experiments) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(experiments)))\n",
    "    \n",
    "    for i, exp in enumerate(experiments):\n",
    "        label = f\"{exp['asset_rid']} (AUC={exp['roc_auc']['micro']:.3f}, Acc={exp['accuracy']:.1f}%)\"\n",
    "        ax.plot(exp['fpr']['micro'], exp['tpr']['micro'], \n",
    "                color=colors[i], linewidth=2, label=label)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve Comparison (Micro-Average)')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Single experiment - no comparison plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CIFAR-10 ROC Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Catalog: {ml.host_name}:{ml.catalog_id}\")\n",
    "print(f\"Execution: {execution.execution_rid}\")\n",
    "print(f\"Ground truth: Execution {gt_execution} ({len(gt_lookup)} labels)\")\n",
    "print(f\"Classes: {n_classes}\")\n",
    "print(f\"Experiments analyzed: {len(experiments)}\")\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n  Asset {exp['asset_rid']}:\")\n",
    "    print(f\"    Samples: {exp['n_samples']}\")\n",
    "    print(f\"    Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"    Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"    Macro-AUC: {exp['roc_auc']['macro']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete execution and upload outputs\n",
    "execution.upload_execution_outputs()\n",
    "print(f\"\\nExecution completed: {execution.execution_rid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
