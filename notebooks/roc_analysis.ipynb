{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis for CIFAR-10 CNN Experiments\n",
    "\n",
    "This notebook computes ROC (Receiver Operating Characteristic) curves and AUC (Area Under Curve) scores for CIFAR-10 CNN classification experiments. It compares model performance across different training configurations.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load prediction probability files from DerivaML catalog as assets\n",
    "2. Retrieve ground truth labels from the Image_Classification feature table\n",
    "3. Compute per-class and micro/macro-averaged ROC curves\n",
    "4. Generate comparison visualizations\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Prediction probability CSV files with columns: `Image_RID`, `Predicted_Class`, `prob_<classname>` for each class\n",
    "- Ground truth labels stored in the Image_Classification feature (from a labeling execution with no confidence scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from deriva_ml.execution import run_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Initialize Notebook\n",
    "\n",
    "Initialize the notebook with DerivaML execution context. This single call:\n",
    "1. Loads all configuration modules\n",
    "2. Resolves the hydra-zen configuration\n",
    "3. Creates the DerivaML connection\n",
    "4. Creates a workflow and execution context\n",
    "5. Downloads any specified assets\n",
    "\n",
    "Override configuration at runtime:\n",
    "```python\n",
    "ml, execution, config = run_notebook(\n",
    "    \"roc_analysis\",\n",
    "    overrides=[\"assets=roc_quick_probabilities\"],  # Analyze single experiment\n",
    ")\n",
    "```\n",
    "\n",
    "Available asset configurations (see `src/configs/assets.py`):\n",
    "- `roc_quick_probabilities` - cifar10_quick experiment only\n",
    "- `roc_extended_probabilities` - cifar10_extended experiment only  \n",
    "- `roc_comparison_probabilities` - both experiments (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize notebook - this single call handles all setup\n",
    "ml, execution, config = run_notebook(\"roc_analysis\", workflow_type=\"ROC Analysis Notebook\")\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"\"\"**Connection:** `{ml.host_name}`, catalog `{ml.catalog_id}`\n",
    "\n",
    "**Execution:** [{execution.execution_rid}]({ml.cite(execution.execution_rid)})\n",
    "\n",
    "**Configuration:**\n",
    "- Assets: `{config.assets}`\n",
    "- Show per-class curves: `{config.show_per_class}`\n",
    "- Confidence threshold: `{config.confidence_threshold}`\n",
    "\n",
    "**Downloaded:** {list(execution.asset_paths.keys())}\n",
    "\"\"\"))\n",
    "\n",
    "# Display asset configuration description\n",
    "if hasattr(config.assets, 'description') and config.assets.description:\n",
    "    display(Markdown(f\"**Asset Configuration Description:**\\n\\n{config.assets.description}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load Experiments from Assets\n",
    "\n",
    "Load the prediction probability CSV files from downloaded assets and create `Experiment` objects to access configuration metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Build Experiment objects for each prediction asset\n",
    "experiments = []\n",
    "loaded_info = []\n",
    "\n",
    "for asset_path in execution.asset_paths.get('Execution_Asset', []):\n",
    "    if asset_path.file_name.name == \"prediction_probabilities.csv\":\n",
    "        # Find the source execution that produced this asset\n",
    "        asset = ml.lookup_asset(asset_path.asset_rid)\n",
    "        asset_executions = asset.list_executions(asset_role='Output')\n",
    "        \n",
    "        if asset_executions:\n",
    "            exec_rid = asset_executions[0]['Execution']\n",
    "            exp = ml.lookup_experiment(exec_rid)\n",
    "            \n",
    "            # Load prediction data\n",
    "            df = pd.read_csv(asset_path.file_name)\n",
    "            \n",
    "            experiments.append({\n",
    "                'experiment': exp,\n",
    "                'asset_rid': asset_path.asset_rid,\n",
    "                'data': df,\n",
    "                'name': exp.name,\n",
    "                'config_choices': exp.config_choices,\n",
    "                'model_config': exp.model_config,\n",
    "            })\n",
    "            loaded_info.append(\n",
    "                f\"- **{exp.name}**: {len(df)} predictions \"\n",
    "                f\"(execution [{exec_rid}]({ml.cite(exec_rid)}))\"\n",
    "            )\n",
    "\n",
    "display(Markdown(\"**Loaded experiments:**\\n\\n\" + \"\\n\".join(loaded_info)))\n",
    "display(Markdown(f\"\\n*Successfully loaded {len(experiments)} experiments*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Experiment Summary\n",
    "\n",
    "Display detailed configuration information for each experiment using the `Experiment` class. This shows:\n",
    "- **Configuration Choices**: The Hydra config names used (e.g., `model_config=cifar10_quick`)\n",
    "- **Model Configuration**: Hyperparameters like epochs, learning rate, batch size\n",
    "- **Input Datasets**: Training and test datasets used\n",
    "- **Input Assets**: Any pre-trained weights or other assets used as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display experiment configurations using Experiment.display_markdown()\n",
    "for exp_data in experiments:\n",
    "    exp_data['experiment'].display_markdown()\n",
    "\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Get Ground Truth Labels\n",
    "\n",
    "Retrieve ground truth labels from the `Image_Classification` feature table. This feature stores classification labels for images, potentially from multiple sources (executions).\n",
    "\n",
    "**Identifying ground truth:**\n",
    "- Ground truth labels are manually assigned and have **no confidence score** (NULL)\n",
    "- Model predictions have confidence scores from softmax probabilities\n",
    "- We identify the ground truth execution by finding labels with zero confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels from the feature table\n",
    "all_feature_values = list(ml.list_feature_values(\"Image\", \"Image_Classification\"))\n",
    "feature_df = pd.DataFrame(all_feature_values)\n",
    "\n",
    "# Ground truth labels have no confidence score (manually labeled)\n",
    "# Group by execution to identify which has ground truth\n",
    "exec_summary = feature_df.groupby('Execution').agg({\n",
    "    'Image': 'count',\n",
    "    'Confidence': lambda x: x.notna().sum()\n",
    "}).rename(columns={'Image': 'num_images', 'Confidence': 'with_confidence'})\n",
    "\n",
    "# Find execution with no confidence scores (ground truth)\n",
    "gt_mask = exec_summary['with_confidence'] == 0\n",
    "if gt_mask.any():\n",
    "    gt_execution = exec_summary[gt_mask].index[0]\n",
    "else:\n",
    "    gt_execution = exec_summary['num_images'].idxmax()\n",
    "\n",
    "# Extract ground truth as lookup dictionary\n",
    "ground_truth = feature_df[feature_df['Execution'] == gt_execution][['Image', 'Image_Class']]\n",
    "gt_lookup = dict(zip(ground_truth['Image'], ground_truth['Image_Class']))\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted(ground_truth['Image_Class'].unique())\n",
    "n_classes = len(class_names)\n",
    "\n",
    "display(Markdown(f\"\"\"**Ground Truth:**\n",
    "- Execution: [{gt_execution}]({ml.cite(gt_execution)})\n",
    "- Total labels: {exec_summary.loc[gt_execution, 'num_images']}\n",
    "- Classes ({n_classes}): {', '.join(f'`{c}`' for c in class_names)}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Merge Predictions with Ground Truth\n",
    "\n",
    "Join prediction data with ground truth labels using `Image_RID` as the key. Only images that have both predictions and ground truth labels will be included in the ROC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ground truth to each experiment's predictions\n",
    "merge_results = []\n",
    "\n",
    "for exp in experiments:\n",
    "    df = exp['data'].copy()\n",
    "    \n",
    "    # Map Image_RID to ground truth class\n",
    "    df['True_Class'] = df['Image_RID'].map(gt_lookup)\n",
    "    \n",
    "    # Keep only images with ground truth\n",
    "    matched = df['True_Class'].notna().sum()\n",
    "    total = len(df)\n",
    "    \n",
    "    df = df.dropna(subset=['True_Class'])\n",
    "    exp['data'] = df\n",
    "    exp['n_samples'] = len(df)\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        exp['accuracy'] = (df['Predicted_Class'] == df['True_Class']).mean() * 100\n",
    "        merge_results.append(f\"- **{exp['name']}**: {matched}/{total} matched, accuracy {exp['accuracy']:.1f}%\")\n",
    "    else:\n",
    "        exp['accuracy'] = float('nan')\n",
    "        merge_results.append(f\"- **{exp['name']}**: ⚠️ No matching samples found!\")\n",
    "\n",
    "display(Markdown(\"**Prediction/Ground Truth Merge:**\\n\\n\" + \"\\n\".join(merge_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Compute ROC Curves\n",
    "\n",
    "For multi-class classification, we use the **one-vs-rest (OvR)** approach:\n",
    "- Each class gets its own ROC curve treating it as positive vs. all others\n",
    "- **Micro-average**: Aggregate all classes, treating each prediction as independent\n",
    "- **Macro-average**: Simple mean of per-class AUC scores (equal weight to each class)\n",
    "\n",
    "AUC (Area Under ROC Curve) ranges from 0.5 (random) to 1.0 (perfect discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_metrics(df: pd.DataFrame, class_names: list[str]) -> dict:\n",
    "    \"\"\"Compute ROC curves and AUC scores for multi-class predictions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with True_Class and prob_* columns\n",
    "        class_names: Ordered list of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dict with fpr, tpr, roc_auc for each class and micro/macro averages\n",
    "    \"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "    \n",
    "    # Convert labels to indices\n",
    "    y_true_idx = df['True_Class'].map(class_to_idx).values\n",
    "    y_true_bin = label_binarize(y_true_idx, classes=range(n_classes))\n",
    "    \n",
    "    # Get probability matrix\n",
    "    prob_cols = [f\"prob_{c}\" for c in class_names]\n",
    "    y_score = df[prob_cols].values\n",
    "    \n",
    "    # Compute per-class ROC\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        fpr[name], tpr[name], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc[name] = auc(fpr[name], tpr[name])\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "    \n",
    "    # Macro-average\n",
    "    roc_auc['macro'] = np.mean([roc_auc[c] for c in class_names])\n",
    "    \n",
    "    return {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC metrics for each experiment\n",
    "roc_results = []\n",
    "\n",
    "for exp in experiments:\n",
    "    metrics = compute_roc_metrics(exp['data'], class_names)\n",
    "    exp.update(metrics)\n",
    "    \n",
    "    roc_results.append(\n",
    "        f\"- **{exp['name']}** (asset [{exp['asset_rid']}]({ml.cite(exp['asset_rid'])})): \"\n",
    "        f\"Accuracy {exp['accuracy']:.2f}%, Micro-AUC {exp['roc_auc']['micro']:.4f}, \"\n",
    "        f\"Macro-AUC {exp['roc_auc']['macro']:.4f}\"\n",
    "    )\n",
    "\n",
    "display(Markdown(\"**ROC Metrics:**\\n\\n\" + \"\\n\".join(roc_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC comparison table\n",
    "if experiments:\n",
    "    auc_data = []\n",
    "    for exp in experiments:\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        row = {'Experiment': exp_name}\n",
    "        for c in class_names:\n",
    "            row[c] = exp['roc_auc'][c]\n",
    "        row['Micro'] = exp['roc_auc']['micro']\n",
    "        row['Macro'] = exp['roc_auc']['macro']\n",
    "        auc_data.append(row)\n",
    "\n",
    "    auc_df = pd.DataFrame(auc_data).set_index('Experiment')\n",
    "    display(Markdown(\"**Per-class AUC scores:**\"))\n",
    "    display(auc_df.round(4))\n",
    "else:\n",
    "    display(Markdown(\"*No experiments loaded*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(exp: dict, class_names: list[str], show_per_class: bool = True):\n",
    "    \"\"\"Plot ROC curves for an experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp: Experiment dict with fpr, tpr, roc_auc data\n",
    "        class_names: List of class names\n",
    "        show_per_class: If True, plot individual class curves. If False, only micro-average.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    fpr, tpr, roc_auc = exp['fpr'], exp['tpr'], exp['roc_auc']\n",
    "    \n",
    "    # Micro-average (always shown)\n",
    "    ax.plot(fpr['micro'], tpr['micro'], \n",
    "            label=f\"Micro-avg (AUC={roc_auc['micro']:.3f})\",\n",
    "            color='deeppink', linestyle=':', linewidth=3)\n",
    "    \n",
    "    # Per-class curves (optional based on config)\n",
    "    if show_per_class:\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "        for i, name in enumerate(class_names):\n",
    "            ax.plot(fpr[name], tpr[name], color=colors[i],\n",
    "                    label=f\"{name} (AUC={roc_auc[name]:.3f})\")\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    \n",
    "    # Use experiment name in title\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    ax.set_title(f\"ROC Curves: {exp_name} (Acc: {exp['accuracy']:.1f}%)\")\n",
    "    \n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each experiment (controlled by config.show_per_class)\n",
    "for exp in experiments:\n",
    "    fig = plot_roc_curves(exp, class_names, show_per_class=config.show_per_class)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Experiment Comparison\n",
    "\n",
    "Compare micro-averaged ROC curves across all experiments. This visualization shows how different model configurations perform relative to each other:\n",
    "- Curves closer to the top-left corner indicate better performance\n",
    "- The diagonal dashed line represents random classification (AUC = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare micro-average ROC curves across experiments\n",
    "if len(experiments) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(experiments)))\n",
    "    \n",
    "    for i, exp in enumerate(experiments):\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        label = f\"{exp_name} (AUC={exp['roc_auc']['micro']:.3f}, Acc={exp['accuracy']:.1f}%)\"\n",
    "        ax.plot(exp['fpr']['micro'], exp['tpr']['micro'], \n",
    "                color=colors[i], linewidth=2, label=label)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve Comparison (Micro-Average)')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    display(Markdown(\"*Single experiment - no comparison plot*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "summary_lines = [\n",
    "    \"# CIFAR-10 ROC Analysis Summary\",\n",
    "    \"\",\n",
    "    f\"**Catalog:** `{ml.host_name}:{ml.catalog_id}`\",\n",
    "    f\"**Execution:** [{execution.execution_rid}]({ml.cite(execution.execution_rid)})\",\n",
    "    f\"**Ground truth:** Execution [{gt_execution}]({ml.cite(gt_execution)}) ({len(gt_lookup)} labels)\",\n",
    "    f\"**Classes:** {n_classes}\",\n",
    "    f\"**Experiments analyzed:** {len(experiments)}\",\n",
    "]\n",
    "\n",
    "display(Markdown(\"\\n\".join(summary_lines)))\n",
    "\n",
    "# Display asset configuration description if available\n",
    "if hasattr(config.assets, 'description') and config.assets.description:\n",
    "    display(Markdown(f\"\\n**Analysis Context:**\\n\\n{config.assets.description}\"))\n",
    "\n",
    "# Display per-experiment summary\n",
    "results_lines = [\"## Experiment Results\", \"\"]\n",
    "for exp in experiments:\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    exp_obj = exp.get('experiment')\n",
    "    \n",
    "    # Get experiment description if available\n",
    "    exp_desc = \"\"\n",
    "    if exp_obj and exp_obj.description:\n",
    "        exp_desc = f\" - {exp_obj.description}\"\n",
    "    \n",
    "    # Link to source execution\n",
    "    exec_link = f\"[{exp_obj.execution_rid}]({ml.cite(exp_obj.execution_rid)})\" if exp_obj else exp['asset_rid']\n",
    "    \n",
    "    results_lines.append(f\"### {exp_name}{exp_desc}\")\n",
    "    results_lines.append(f\"- **Execution:** {exec_link}\")\n",
    "    results_lines.append(f\"- **Samples:** {exp['n_samples']}\")\n",
    "    results_lines.append(f\"- **Accuracy:** {exp['accuracy']:.2f}%\")\n",
    "    results_lines.append(f\"- **Micro-AUC:** {exp['roc_auc']['micro']:.4f}\")\n",
    "    results_lines.append(f\"- **Macro-AUC:** {exp['roc_auc']['macro']:.4f}\")\n",
    "    results_lines.append(\"\")\n",
    "\n",
    "display(Markdown(\"\\n\".join(results_lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete execution and upload outputs\n",
    "execution.upload_execution_outputs()\n",
    "display(Markdown(f\"**Execution completed:** [{execution.execution_rid}]({ml.cite(execution.execution_rid)})\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
