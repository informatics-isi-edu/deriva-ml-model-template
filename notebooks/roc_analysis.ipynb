{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis for CIFAR-10 CNN Experiments\n",
    "\n",
    "This notebook computes ROC (Receiver Operating Characteristic) curves and AUC (Area Under Curve) scores for CIFAR-10 CNN classification experiments. It compares model performance across different training configurations.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load prediction probability files from DerivaML catalog as assets\n",
    "2. Retrieve ground truth labels from the Image_Classification feature table\n",
    "3. Compute per-class and micro/macro-averaged ROC curves\n",
    "4. Generate comparison visualizations\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Prediction probability CSV files with columns: `Image_RID`, `Predicted_Class`, `prob_<classname>` for each class\n",
    "- Ground truth labels stored in the Image_Classification feature (from a labeling execution with no confidence scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from deriva_ml import Experiment\n",
    "from deriva_ml.execution import run_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Initialize Notebook\n",
    "\n",
    "Initialize the notebook with DerivaML execution context. This single call:\n",
    "1. Loads all configuration modules\n",
    "2. Resolves the hydra-zen configuration\n",
    "3. Creates the DerivaML connection\n",
    "4. Creates a workflow and execution context\n",
    "5. Downloads any specified assets\n",
    "\n",
    "Override configuration at runtime:\n",
    "```python\n",
    "ml, execution, config = run_notebook(\n",
    "    \"roc_analysis\",\n",
    "    overrides=[\"assets=roc_quick_probabilities\"],  # Analyze single experiment\n",
    ")\n",
    "```\n",
    "\n",
    "Available asset configurations (see `src/configs/assets.py`):\n",
    "- `roc_quick_probabilities` - cifar10_quick experiment only\n",
    "- `roc_extended_probabilities` - cifar10_extended experiment only  \n",
    "- `roc_comparison_probabilities` - both experiments (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize notebook - this single call handles all setup\n",
    "ml, execution, config = run_notebook(\"roc_analysis\", workflow_type=\"ROC Analysis Notebook\")\n",
    "\n",
    "print(f\"Connected to {ml.host_name}, catalog {ml.catalog_id}\")\n",
    "print(f\"Execution: {execution.execution_rid}\")\n",
    "print(f\"Assets: {config.assets}\")\n",
    "print(f\"Show per-class curves: {config.show_per_class}\")\n",
    "print(f\"Confidence threshold: {config.confidence_threshold}\")\n",
    "print(f\"Downloaded: {list(execution.asset_paths.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load Probability Files from Assets\n",
    "\n",
    "Load the prediction probability CSV files from the downloaded assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probability files from downloaded assets\n",
    "experiments = []\n",
    "for asset_table, asset_list in execution.asset_paths.items():\n",
    "    for asset_path in asset_list:\n",
    "        if asset_path.file_name.name == \"prediction_probabilities.csv\":\n",
    "            df = pd.read_csv(asset_path.file_name)\n",
    "            experiments.append({\n",
    "                'asset_rid': asset_path.asset_rid,\n",
    "                'file_name': asset_path.file_name.name,\n",
    "                'data': df\n",
    "            })\n",
    "            print(f\"Loaded {len(df)} predictions from asset {asset_path.asset_rid}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Build Experiment objects for each prediction asset\n",
    "# Assets are paired: [predictions_csv, config_yaml, predictions_csv, config_yaml, ...]\n",
    "experiments = []\n",
    "asset_rids = config.assets\n",
    "\n",
    "for i, asset_path in enumerate(execution.asset_paths.get('Execution_Asset', [])):\n",
    "    if asset_path.file_name.name == \"prediction_probabilities.csv\":\n",
    "        # Find the source execution that produced this asset\n",
    "        asset = ml.lookup_asset(asset_path.asset_rid)\n",
    "        asset_executions = asset.list_executions(asset_role='Output')\n",
    "        \n",
    "        if asset_executions:\n",
    "            exec_rid = asset_executions[0]['Execution']\n",
    "            exp = Experiment(ml, exec_rid)\n",
    "            \n",
    "            # Load prediction data\n",
    "            df = pd.read_csv(asset_path.file_name)\n",
    "            \n",
    "            experiments.append({\n",
    "                'experiment': exp,\n",
    "                'asset_rid': asset_path.asset_rid,\n",
    "                'data': df,\n",
    "                'name': exp.name,\n",
    "                'config_choices': exp.config_choices,\n",
    "                'model_config': exp.model_config,\n",
    "            })\n",
    "            print(f\"Loaded {len(df)} predictions from {exp.name} (execution {exec_rid})\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(experiments)} experiments\")\n",
    "\n",
    "# Display experiment configurations using Experiment class\n",
    "display(Markdown(\"## Experiment Configurations\"))\n",
    "\n",
    "for exp_data in experiments:\n",
    "    exp = exp_data['experiment']\n",
    "    \n",
    "    # Header with execution link\n",
    "    display(HTML(f\"<hr/><h3>{exp.name} (<a href='{exp.get_chaise_url()}' target='_blank'>{exp.execution_rid}</a>)</h3>\"))\n",
    "    \n",
    "    # Description\n",
    "    if exp.description:\n",
    "        display(Markdown(f\"**Description:** {exp.description}\"))\n",
    "    \n",
    "    # Config choices\n",
    "    if exp.config_choices:\n",
    "        display(Markdown(\"**Configuration Choices:**\"))\n",
    "        choices_str = \", \".join(f\"{k}={v}\" for k, v in sorted(exp.config_choices.items()))\n",
    "        display(HTML(f\"<code>{choices_str}</code>\"))\n",
    "    \n",
    "    # Model configuration\n",
    "    model_cfg = {k: v for k, v in exp.model_config.items() if not k.startswith('_')}\n",
    "    if model_cfg:\n",
    "        config_df = pd.DataFrame([{'Parameter': k, 'Value': v} for k, v in sorted(model_cfg.items())]).set_index('Parameter')\n",
    "        display(Markdown(\"**Model Configuration:**\"))\n",
    "        display(config_df)\n",
    "    \n",
    "    # Input datasets\n",
    "    if exp.input_datasets:\n",
    "        display(Markdown(\"**Input Datasets:**\"))\n",
    "        for ds in exp.input_datasets:\n",
    "            url = f\"https://{ml.host_name}/chaise/record/#{ml.catalog_id}/deriva-ml:Dataset/RID={ds.dataset_rid}\"\n",
    "            types_str = f\" [{', '.join(ds.dataset_types)}]\" if ds.dataset_types else \"\"\n",
    "            display(HTML(f\"• <a href='{url}' target='_blank'>{ds.dataset_rid}</a> v{ds.current_version}{types_str}\"))\n",
    "    \n",
    "    # Input assets\n",
    "    if exp.input_assets:\n",
    "        display(Markdown(\"**Input Assets:**\"))\n",
    "        for asset in exp.input_assets:\n",
    "            display(HTML(f\"• <a href='{asset.get_chaise_url()}' target='_blank'>{asset.asset_rid}</a> — {asset.filename}\"))\n",
    "\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Get Ground Truth Labels\n",
    "\n",
    "Retrieve ground truth labels from the `Image_Classification` feature table. This feature stores classification labels for images, potentially from multiple sources (executions).\n",
    "\n",
    "**Identifying ground truth:**\n",
    "- Ground truth labels are manually assigned and have **no confidence score** (NULL)\n",
    "- Model predictions have confidence scores from softmax probabilities\n",
    "- We identify the ground truth execution by finding labels with zero confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels from the feature table\n",
    "all_feature_values = list(ml.list_feature_values(\"Image\", \"Image_Classification\"))\n",
    "feature_df = pd.DataFrame(all_feature_values)\n",
    "\n",
    "# Ground truth labels have no confidence score (manually labeled)\n",
    "# Group by execution to identify which has ground truth\n",
    "exec_summary = feature_df.groupby('Execution').agg({\n",
    "    'Image': 'count',\n",
    "    'Confidence': lambda x: x.notna().sum()\n",
    "}).rename(columns={'Image': 'num_images', 'Confidence': 'with_confidence'})\n",
    "\n",
    "# Find execution with no confidence scores (ground truth)\n",
    "gt_mask = exec_summary['with_confidence'] == 0\n",
    "if gt_mask.any():\n",
    "    gt_execution = exec_summary[gt_mask].index[0]\n",
    "else:\n",
    "    gt_execution = exec_summary['num_images'].idxmax()\n",
    "    \n",
    "print(f\"Ground truth execution: {gt_execution}\")\n",
    "print(f\"Total ground truth labels: {exec_summary.loc[gt_execution, 'num_images']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth as lookup dictionary\n",
    "ground_truth = feature_df[feature_df['Execution'] == gt_execution][['Image', 'Image_Class']]\n",
    "gt_lookup = dict(zip(ground_truth['Image'], ground_truth['Image_Class']))\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted(ground_truth['Image_Class'].unique())\n",
    "n_classes = len(class_names)\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Merge Predictions with Ground Truth\n",
    "\n",
    "Join prediction data with ground truth labels using `Image_RID` as the key. Only images that have both predictions and ground truth labels will be included in the ROC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ground truth to each experiment's predictions\n",
    "for exp in experiments:\n",
    "    df = exp['data'].copy()\n",
    "    \n",
    "    # Debug: show sample RIDs from predictions vs ground truth\n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Prediction Image_RIDs (first 5): {df['Image_RID'].head().tolist()}\")\n",
    "    print(f\"  Ground truth Image keys (first 5): {list(gt_lookup.keys())[:5]}\")\n",
    "    \n",
    "    df['True_Class'] = df['Image_RID'].map(gt_lookup)\n",
    "    # Keep only images with ground truth\n",
    "    matched = df['True_Class'].notna().sum()\n",
    "    print(f\"  Matched: {matched} / {len(df)}\")\n",
    "    \n",
    "    df = df.dropna(subset=['True_Class'])\n",
    "    exp['data'] = df\n",
    "    exp['n_samples'] = len(df)\n",
    "    if len(df) > 0:\n",
    "        exp['accuracy'] = (df['Predicted_Class'] == df['True_Class']).mean() * 100\n",
    "        print(f\"  Accuracy: {exp['accuracy']:.1f}%\")\n",
    "    else:\n",
    "        exp['accuracy'] = float('nan')\n",
    "        print(\"  No matching samples found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Compute ROC Curves\n",
    "\n",
    "For multi-class classification, we use the **one-vs-rest (OvR)** approach:\n",
    "- Each class gets its own ROC curve treating it as positive vs. all others\n",
    "- **Micro-average**: Aggregate all classes, treating each prediction as independent\n",
    "- **Macro-average**: Simple mean of per-class AUC scores (equal weight to each class)\n",
    "\n",
    "AUC (Area Under ROC Curve) ranges from 0.5 (random) to 1.0 (perfect discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_metrics(df: pd.DataFrame, class_names: list[str]) -> dict:\n",
    "    \"\"\"Compute ROC curves and AUC scores for multi-class predictions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with True_Class and prob_* columns\n",
    "        class_names: Ordered list of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dict with fpr, tpr, roc_auc for each class and micro/macro averages\n",
    "    \"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "    \n",
    "    # Convert labels to indices\n",
    "    y_true_idx = df['True_Class'].map(class_to_idx).values\n",
    "    y_true_bin = label_binarize(y_true_idx, classes=range(n_classes))\n",
    "    \n",
    "    # Get probability matrix\n",
    "    prob_cols = [f\"prob_{c}\" for c in class_names]\n",
    "    y_score = df[prob_cols].values\n",
    "    \n",
    "    # Compute per-class ROC\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        fpr[name], tpr[name], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc[name] = auc(fpr[name], tpr[name])\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "    \n",
    "    # Macro-average\n",
    "    roc_auc['macro'] = np.mean([roc_auc[c] for c in class_names])\n",
    "    \n",
    "    return {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC metrics for each experiment\n",
    "for exp in experiments:\n",
    "    metrics = compute_roc_metrics(exp['data'], class_names)\n",
    "    exp.update(metrics)\n",
    "    \n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"  Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"  Macro-AUC: {exp['roc_auc']['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC comparison table\n",
    "if experiments:\n",
    "    auc_data = []\n",
    "    for exp in experiments:\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        row = {'Experiment': exp_name}\n",
    "        for c in class_names:\n",
    "            row[c] = exp['roc_auc'][c]\n",
    "        row['Micro'] = exp['roc_auc']['micro']\n",
    "        row['Macro'] = exp['roc_auc']['macro']\n",
    "        auc_data.append(row)\n",
    "\n",
    "    auc_df = pd.DataFrame(auc_data).set_index('Experiment')\n",
    "    print(\"\\nPer-class AUC scores:\")\n",
    "    display(auc_df.round(4))\n",
    "else:\n",
    "    print(\"No experiments loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(exp: dict, class_names: list[str], show_per_class: bool = True):\n",
    "    \"\"\"Plot ROC curves for an experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp: Experiment dict with fpr, tpr, roc_auc data\n",
    "        class_names: List of class names\n",
    "        show_per_class: If True, plot individual class curves. If False, only micro-average.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    fpr, tpr, roc_auc = exp['fpr'], exp['tpr'], exp['roc_auc']\n",
    "    \n",
    "    # Micro-average (always shown)\n",
    "    ax.plot(fpr['micro'], tpr['micro'], \n",
    "            label=f\"Micro-avg (AUC={roc_auc['micro']:.3f})\",\n",
    "            color='deeppink', linestyle=':', linewidth=3)\n",
    "    \n",
    "    # Per-class curves (optional based on config)\n",
    "    if show_per_class:\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "        for i, name in enumerate(class_names):\n",
    "            ax.plot(fpr[name], tpr[name], color=colors[i],\n",
    "                    label=f\"{name} (AUC={roc_auc[name]:.3f})\")\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    \n",
    "    # Use experiment name in title\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    ax.set_title(f\"ROC Curves: {exp_name} (Acc: {exp['accuracy']:.1f}%)\")\n",
    "    \n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each experiment (controlled by config.show_per_class)\n",
    "for exp in experiments:\n",
    "    # Display experiment header with link to source execution\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    if 'execution_url' in exp:\n",
    "        display(HTML(f\"<h3>{exp_name}</h3><p>Source: <a href='{exp['execution_url']}' target='_blank'>Execution {exp['source_execution']}</a></p>\"))\n",
    "    else:\n",
    "        display(HTML(f\"<h3>{exp_name}</h3>\"))\n",
    "    \n",
    "    fig = plot_roc_curves(exp, class_names, show_per_class=config.show_per_class)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Experiment Comparison\n",
    "\n",
    "Compare micro-averaged ROC curves across all experiments. This visualization shows how different model configurations perform relative to each other:\n",
    "- Curves closer to the top-left corner indicate better performance\n",
    "- The diagonal dashed line represents random classification (AUC = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare micro-average ROC curves across experiments\n",
    "if len(experiments) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(experiments)))\n",
    "    \n",
    "    for i, exp in enumerate(experiments):\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        label = f\"{exp_name} (AUC={exp['roc_auc']['micro']:.3f}, Acc={exp['accuracy']:.1f}%)\"\n",
    "        ax.plot(exp['fpr']['micro'], exp['tpr']['micro'], \n",
    "                color=colors[i], linewidth=2, label=label)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve Comparison (Micro-Average)')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Single experiment - no comparison plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CIFAR-10 ROC Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Catalog: {ml.host_name}:{ml.catalog_id}\")\n",
    "print(f\"Execution: {execution.execution_rid}\")\n",
    "print(f\"Ground truth: Execution {gt_execution} ({len(gt_lookup)} labels)\")\n",
    "print(f\"Classes: {n_classes}\")\n",
    "print(f\"Experiments analyzed: {len(experiments)}\")\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    print(f\"\\n  {exp_name}:\")\n",
    "    if 'source_execution' in exp:\n",
    "        print(f\"    Source Execution: {exp['source_execution']}\")\n",
    "    print(f\"    Samples: {exp['n_samples']}\")\n",
    "    print(f\"    Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"    Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"    Macro-AUC: {exp['roc_auc']['macro']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete execution and upload outputs\n",
    "execution.upload_execution_outputs()\n",
    "print(f\"\\nExecution completed: {execution.execution_rid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
