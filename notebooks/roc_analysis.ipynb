{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis for CIFAR-10 CNN Experiments\n",
    "\n",
    "This notebook computes ROC (Receiver Operating Characteristic) curves and AUC (Area Under Curve) scores for CIFAR-10 CNN classification experiments. It compares model performance across different training configurations.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load prediction probability files from DerivaML catalog as assets\n",
    "2. Retrieve ground truth labels from the Image_Classification feature table\n",
    "3. Compute per-class and micro/macro-averaged ROC curves\n",
    "4. Generate comparison visualizations\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Prediction probability CSV files with columns: `Image_RID`, `Predicted_Class`, `prob_<classname>` for each class\n",
    "- Ground truth labels stored in the Image_Classification feature (from a labeling execution with no confidence scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from deriva_ml.execution import run_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Initialize Notebook\n",
    "\n",
    "Initialize the notebook with DerivaML execution context. This single call:\n",
    "1. Loads all configuration modules\n",
    "2. Resolves the hydra-zen configuration\n",
    "3. Creates the DerivaML connection\n",
    "4. Creates a workflow and execution context\n",
    "5. Downloads any specified assets\n",
    "\n",
    "Override configuration at runtime:\n",
    "```python\n",
    "ml, execution, config = run_notebook(\n",
    "    \"roc_analysis\",\n",
    "    overrides=[\"assets=roc_quick_probabilities\"],  # Analyze single experiment\n",
    ")\n",
    "```\n",
    "\n",
    "Available asset configurations (see `src/configs/assets.py`):\n",
    "- `roc_quick_probabilities` - cifar10_quick experiment only\n",
    "- `roc_extended_probabilities` - cifar10_extended experiment only  \n",
    "- `roc_comparison_probabilities` - both experiments (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize notebook - this single call handles all setup\n",
    "ml, execution, config = run_notebook(\"roc_analysis\", workflow_type=\"ROC Analysis Notebook\")\n",
    "\n",
    "print(f\"Connected to {ml.host_name}, catalog {ml.catalog_id}\")\n",
    "print(f\"Execution: {execution.execution_rid}\")\n",
    "print(f\"Assets: {config.assets}\")\n",
    "print(f\"Show per-class curves: {config.show_per_class}\")\n",
    "print(f\"Confidence threshold: {config.confidence_threshold}\")\n",
    "print(f\"Downloaded: {list(execution.asset_paths.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load Probability Files from Assets\n",
    "\n",
    "Load the prediction probability CSV files from the downloaded assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probability files from downloaded assets\n",
    "experiments = []\n",
    "for asset_table, asset_list in execution.asset_paths.items():\n",
    "    for asset_path in asset_list:\n",
    "        if asset_path.file_name.name == \"prediction_probabilities.csv\":\n",
    "            df = pd.read_csv(asset_path.file_name)\n",
    "            experiments.append({\n",
    "                'asset_rid': asset_path.asset_rid,\n",
    "                'file_name': asset_path.file_name.name,\n",
    "                'data': df\n",
    "            })\n",
    "            print(f\"Loaded {len(df)} predictions from asset {asset_path.asset_rid}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Load hydra config YAML files from downloaded assets\n",
    "configs = {}\n",
    "for asset_table, asset_list in execution.asset_paths.items():\n",
    "    for asset_path in asset_list:\n",
    "        if asset_path.file_name.suffix in ['.yaml', '.yml']:\n",
    "            with open(asset_path.file_name) as f:\n",
    "                configs[asset_path.asset_rid] = yaml.safe_load(f)\n",
    "\n",
    "def derive_experiment_name(cfg: dict) -> str:\n",
    "    \"\"\"Derive a descriptive experiment name from model configuration.\n",
    "    \n",
    "    Uses distinguishing hyperparameters to create a meaningful name.\n",
    "    \"\"\"\n",
    "    mc = cfg.get('model_config', {})\n",
    "    parts = []\n",
    "    \n",
    "    # Architecture indicators\n",
    "    if mc.get('conv1_channels', 32) != 32 or mc.get('conv2_channels', 64) != 64:\n",
    "        parts.append(f\"c{mc.get('conv1_channels', 32)}-{mc.get('conv2_channels', 64)}\")\n",
    "    \n",
    "    # Training indicators\n",
    "    epochs = mc.get('epochs', 10)\n",
    "    if epochs <= 5:\n",
    "        parts.append('quick')\n",
    "    elif epochs >= 30:\n",
    "        parts.append('extended')\n",
    "    \n",
    "    # Regularization\n",
    "    if mc.get('dropout_rate', 0) > 0:\n",
    "        parts.append('dropout')\n",
    "    if mc.get('weight_decay', 0) > 0:\n",
    "        parts.append('decay')\n",
    "    \n",
    "    return '_'.join(parts) if parts else 'default'\n",
    "\n",
    "# Match configs to experiments and extract experiment names\n",
    "asset_rids = config.assets\n",
    "for exp in experiments:\n",
    "    pred_rid = exp['asset_rid']\n",
    "    if pred_rid in asset_rids:\n",
    "        pred_idx = asset_rids.index(pred_rid)\n",
    "        if pred_idx + 1 < len(asset_rids):\n",
    "            config_rid = asset_rids[pred_idx + 1]\n",
    "            if config_rid in configs:\n",
    "                exp['config'] = configs[config_rid]\n",
    "                exp['config_rid'] = config_rid\n",
    "                \n",
    "                # Derive experiment name from configuration\n",
    "                exp['name'] = derive_experiment_name(configs[config_rid])\n",
    "\n",
    "# Find source execution details for each prediction asset\n",
    "pb = ml.pathBuilder()\n",
    "ml_schema = pb.schemas['deriva-ml']\n",
    "\n",
    "for exp in experiments:\n",
    "    # Use list_asset_executions API to find which execution produced this asset\n",
    "    try:\n",
    "        asset_executions = ml.list_asset_executions(exp['asset_rid'], asset_role='Output')\n",
    "        if asset_executions:\n",
    "            exec_rid = asset_executions[0]['Execution']\n",
    "            exp['source_execution'] = exec_rid\n",
    "            exp['execution_url'] = f\"https://{ml.host_name}/chaise/record/#{ml.catalog_id}/deriva-ml:Execution/RID={exec_rid}\"\n",
    "            \n",
    "            # Get execution details using lookup_execution\n",
    "            exec_obj = ml.lookup_execution(exec_rid)\n",
    "            exp['execution_record'] = {\n",
    "                'Description': exec_obj.configuration.description if exec_obj.configuration else '',\n",
    "                'Status': exec_obj.status.value if exec_obj.status else '',\n",
    "            }\n",
    "            \n",
    "            # Get input datasets from execution\n",
    "            dataset_exec_table = ml_schema.tables['Dataset_Execution']\n",
    "            dataset_links = list(dataset_exec_table.filter(dataset_exec_table.Execution == exec_rid).entities())\n",
    "            \n",
    "            exp['input_datasets'] = []\n",
    "            for link in dataset_links:\n",
    "                dataset_rid = link['Dataset']\n",
    "                try:\n",
    "                    ds = ml.lookup_dataset(dataset_rid)\n",
    "                    exp['input_datasets'].append({\n",
    "                        'rid': ds.dataset_rid,\n",
    "                        'version': str(ds.current_version),\n",
    "                        'description': ds.description,\n",
    "                        'types': ds.dataset_types,\n",
    "                        'url': f\"https://{ml.host_name}/chaise/record/#{ml.catalog_id}/deriva-ml:Dataset/RID={ds.dataset_rid}\"\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    exp['input_datasets'].append({'rid': dataset_rid, 'error': str(e)})\n",
    "            \n",
    "            # Get input assets from the execution (not from the prediction asset)\n",
    "            asset_exec_table = ml_schema.tables['Execution_Asset_Execution']\n",
    "            input_asset_rows = list(asset_exec_table.filter(\n",
    "                (asset_exec_table.Execution == exec_rid) & \n",
    "                (asset_exec_table.Asset_Role == 'Input')\n",
    "            ).entities())\n",
    "            \n",
    "            exp['input_assets'] = []\n",
    "            for row in input_asset_rows:\n",
    "                asset_rid = row['Execution_Asset']\n",
    "                try:\n",
    "                    asset = ml.lookup_asset(asset_rid)\n",
    "                    exp['input_assets'].append({\n",
    "                        'rid': asset_rid,\n",
    "                        'filename': asset.filename,\n",
    "                        'url': asset.get_chaise_url()\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    exp['input_assets'].append({'rid': asset_rid, 'filename': str(e), 'url': ''})\n",
    "                    \n",
    "    except Exception as e:\n",
    "        # Fallback to direct query if API fails\n",
    "        print(f\"Warning: list_asset_executions failed for {exp['asset_rid']}: {e}\")\n",
    "        asset_exec_table = ml_schema.tables['Execution_Asset_Execution']\n",
    "        rows = list(asset_exec_table.filter(\n",
    "            (asset_exec_table.Execution_Asset == exp['asset_rid']) & \n",
    "            (asset_exec_table.Asset_Role == 'Output')\n",
    "        ).entities())\n",
    "        \n",
    "        if rows:\n",
    "            exec_rid = rows[0]['Execution']\n",
    "            exp['source_execution'] = exec_rid\n",
    "            exp['execution_url'] = f\"https://{ml.host_name}/chaise/record/#{ml.catalog_id}/deriva-ml:Execution/RID={exec_rid}\"\n",
    "\n",
    "# Display each experiment's information sequentially\n",
    "display(Markdown(\"## Experiment Configurations\"))\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    \n",
    "    # 1) Experiment name with execution RID as link in title\n",
    "    if 'execution_url' in exp:\n",
    "        display(HTML(f\"<hr/><h3>{exp_name} (<a href='{exp['execution_url']}' target='_blank'>{exp['source_execution']}</a>)</h3>\"))\n",
    "    else:\n",
    "        display(Markdown(f\"---\\n### {exp_name}\"))\n",
    "    \n",
    "    # 2) Experiment description\n",
    "    if 'execution_record' in exp and exp['execution_record'].get('Description'):\n",
    "        display(Markdown(f\"**Description:** {exp['execution_record']['Description']}\"))\n",
    "    \n",
    "    # 3) Table of model configuration parameters\n",
    "    if 'config' in exp and 'model_config' in exp['config']:\n",
    "        mc = exp['config']['model_config']\n",
    "        config_rows = []\n",
    "        for key, val in sorted(mc.items()):\n",
    "            if not key.startswith('_'):\n",
    "                config_rows.append({'Parameter': key, 'Value': val})\n",
    "        \n",
    "        if config_rows:\n",
    "            config_df = pd.DataFrame(config_rows).set_index('Parameter')\n",
    "            display(Markdown(\"**Model Configuration:**\"))\n",
    "            display(config_df)\n",
    "    \n",
    "    # 4) List of input datasets with version and descriptions\n",
    "    if exp.get('input_datasets'):\n",
    "        display(Markdown(\"**Input Datasets:**\"))\n",
    "        for ds in exp['input_datasets']:\n",
    "            if 'error' in ds:\n",
    "                display(HTML(f\"• {ds['rid']} (error: {ds['error']})\"))\n",
    "            else:\n",
    "                version_str = f\" v{ds['version']}\" if ds.get('version') else \"\"\n",
    "                types_str = f\" [{', '.join(ds['types'])}]\" if ds.get('types') else \"\"\n",
    "                desc_str = f\"<br/>&nbsp;&nbsp;&nbsp;&nbsp;{ds['description']}\" if ds.get('description') else \"\"\n",
    "                display(HTML(f\"• <a href='{ds['url']}' target='_blank'>{ds['rid']}</a>{version_str}{types_str}{desc_str}\"))\n",
    "    \n",
    "    # 5) Table of input asset RIDs\n",
    "    if exp.get('input_assets'):\n",
    "        display(Markdown(\"**Input Assets:**\"))\n",
    "        for asset in exp['input_assets']:\n",
    "            display(HTML(f\"• <a href='{asset['url']}' target='_blank'>{asset['rid']}</a> — {asset['filename']}\"))\n",
    "    elif 'input_assets' in exp:\n",
    "        display(Markdown(\"**Input Assets:** None\"))\n",
    "\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Get Ground Truth Labels\n",
    "\n",
    "Retrieve ground truth labels from the `Image_Classification` feature table. This feature stores classification labels for images, potentially from multiple sources (executions).\n",
    "\n",
    "**Identifying ground truth:**\n",
    "- Ground truth labels are manually assigned and have **no confidence score** (NULL)\n",
    "- Model predictions have confidence scores from softmax probabilities\n",
    "- We identify the ground truth execution by finding labels with zero confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels from the feature table\n",
    "all_feature_values = list(ml.list_feature_values(\"Image\", \"Image_Classification\"))\n",
    "feature_df = pd.DataFrame(all_feature_values)\n",
    "\n",
    "# Ground truth labels have no confidence score (manually labeled)\n",
    "# Group by execution to identify which has ground truth\n",
    "exec_summary = feature_df.groupby('Execution').agg({\n",
    "    'Image': 'count',\n",
    "    'Confidence': lambda x: x.notna().sum()\n",
    "}).rename(columns={'Image': 'num_images', 'Confidence': 'with_confidence'})\n",
    "\n",
    "# Find execution with no confidence scores (ground truth)\n",
    "gt_mask = exec_summary['with_confidence'] == 0\n",
    "if gt_mask.any():\n",
    "    gt_execution = exec_summary[gt_mask].index[0]\n",
    "else:\n",
    "    gt_execution = exec_summary['num_images'].idxmax()\n",
    "    \n",
    "print(f\"Ground truth execution: {gt_execution}\")\n",
    "print(f\"Total ground truth labels: {exec_summary.loc[gt_execution, 'num_images']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth as lookup dictionary\n",
    "ground_truth = feature_df[feature_df['Execution'] == gt_execution][['Image', 'Image_Class']]\n",
    "gt_lookup = dict(zip(ground_truth['Image'], ground_truth['Image_Class']))\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted(ground_truth['Image_Class'].unique())\n",
    "n_classes = len(class_names)\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Merge Predictions with Ground Truth\n",
    "\n",
    "Join prediction data with ground truth labels using `Image_RID` as the key. Only images that have both predictions and ground truth labels will be included in the ROC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ground truth to each experiment's predictions\n",
    "for exp in experiments:\n",
    "    df = exp['data'].copy()\n",
    "    \n",
    "    # Debug: show sample RIDs from predictions vs ground truth\n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Prediction Image_RIDs (first 5): {df['Image_RID'].head().tolist()}\")\n",
    "    print(f\"  Ground truth Image keys (first 5): {list(gt_lookup.keys())[:5]}\")\n",
    "    \n",
    "    df['True_Class'] = df['Image_RID'].map(gt_lookup)\n",
    "    # Keep only images with ground truth\n",
    "    matched = df['True_Class'].notna().sum()\n",
    "    print(f\"  Matched: {matched} / {len(df)}\")\n",
    "    \n",
    "    df = df.dropna(subset=['True_Class'])\n",
    "    exp['data'] = df\n",
    "    exp['n_samples'] = len(df)\n",
    "    if len(df) > 0:\n",
    "        exp['accuracy'] = (df['Predicted_Class'] == df['True_Class']).mean() * 100\n",
    "        print(f\"  Accuracy: {exp['accuracy']:.1f}%\")\n",
    "    else:\n",
    "        exp['accuracy'] = float('nan')\n",
    "        print(\"  No matching samples found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Compute ROC Curves\n",
    "\n",
    "For multi-class classification, we use the **one-vs-rest (OvR)** approach:\n",
    "- Each class gets its own ROC curve treating it as positive vs. all others\n",
    "- **Micro-average**: Aggregate all classes, treating each prediction as independent\n",
    "- **Macro-average**: Simple mean of per-class AUC scores (equal weight to each class)\n",
    "\n",
    "AUC (Area Under ROC Curve) ranges from 0.5 (random) to 1.0 (perfect discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_metrics(df: pd.DataFrame, class_names: list[str]) -> dict:\n",
    "    \"\"\"Compute ROC curves and AUC scores for multi-class predictions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with True_Class and prob_* columns\n",
    "        class_names: Ordered list of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dict with fpr, tpr, roc_auc for each class and micro/macro averages\n",
    "    \"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "    \n",
    "    # Convert labels to indices\n",
    "    y_true_idx = df['True_Class'].map(class_to_idx).values\n",
    "    y_true_bin = label_binarize(y_true_idx, classes=range(n_classes))\n",
    "    \n",
    "    # Get probability matrix\n",
    "    prob_cols = [f\"prob_{c}\" for c in class_names]\n",
    "    y_score = df[prob_cols].values\n",
    "    \n",
    "    # Compute per-class ROC\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        fpr[name], tpr[name], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc[name] = auc(fpr[name], tpr[name])\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "    \n",
    "    # Macro-average\n",
    "    roc_auc['macro'] = np.mean([roc_auc[c] for c in class_names])\n",
    "    \n",
    "    return {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC metrics for each experiment\n",
    "for exp in experiments:\n",
    "    metrics = compute_roc_metrics(exp['data'], class_names)\n",
    "    exp.update(metrics)\n",
    "    \n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"  Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"  Macro-AUC: {exp['roc_auc']['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC comparison table\n",
    "if experiments:\n",
    "    auc_data = []\n",
    "    for exp in experiments:\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        row = {'Experiment': exp_name}\n",
    "        for c in class_names:\n",
    "            row[c] = exp['roc_auc'][c]\n",
    "        row['Micro'] = exp['roc_auc']['micro']\n",
    "        row['Macro'] = exp['roc_auc']['macro']\n",
    "        auc_data.append(row)\n",
    "\n",
    "    auc_df = pd.DataFrame(auc_data).set_index('Experiment')\n",
    "    print(\"\\nPer-class AUC scores:\")\n",
    "    display(auc_df.round(4))\n",
    "else:\n",
    "    print(\"No experiments loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(exp: dict, class_names: list[str], show_per_class: bool = True):\n",
    "    \"\"\"Plot ROC curves for an experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp: Experiment dict with fpr, tpr, roc_auc data\n",
    "        class_names: List of class names\n",
    "        show_per_class: If True, plot individual class curves. If False, only micro-average.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    fpr, tpr, roc_auc = exp['fpr'], exp['tpr'], exp['roc_auc']\n",
    "    \n",
    "    # Micro-average (always shown)\n",
    "    ax.plot(fpr['micro'], tpr['micro'], \n",
    "            label=f\"Micro-avg (AUC={roc_auc['micro']:.3f})\",\n",
    "            color='deeppink', linestyle=':', linewidth=3)\n",
    "    \n",
    "    # Per-class curves (optional based on config)\n",
    "    if show_per_class:\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "        for i, name in enumerate(class_names):\n",
    "            ax.plot(fpr[name], tpr[name], color=colors[i],\n",
    "                    label=f\"{name} (AUC={roc_auc[name]:.3f})\")\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    \n",
    "    # Use experiment name in title\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    ax.set_title(f\"ROC Curves: {exp_name} (Acc: {exp['accuracy']:.1f}%)\")\n",
    "    \n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each experiment (controlled by config.show_per_class)\n",
    "for exp in experiments:\n",
    "    # Display experiment header with link to source execution\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    if 'execution_url' in exp:\n",
    "        display(HTML(f\"<h3>{exp_name}</h3><p>Source: <a href='{exp['execution_url']}' target='_blank'>Execution {exp['source_execution']}</a></p>\"))\n",
    "    else:\n",
    "        display(HTML(f\"<h3>{exp_name}</h3>\"))\n",
    "    \n",
    "    fig = plot_roc_curves(exp, class_names, show_per_class=config.show_per_class)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Experiment Comparison\n",
    "\n",
    "Compare micro-averaged ROC curves across all experiments. This visualization shows how different model configurations perform relative to each other:\n",
    "- Curves closer to the top-left corner indicate better performance\n",
    "- The diagonal dashed line represents random classification (AUC = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare micro-average ROC curves across experiments\n",
    "if len(experiments) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(experiments)))\n",
    "    \n",
    "    for i, exp in enumerate(experiments):\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        label = f\"{exp_name} (AUC={exp['roc_auc']['micro']:.3f}, Acc={exp['accuracy']:.1f}%)\"\n",
    "        ax.plot(exp['fpr']['micro'], exp['tpr']['micro'], \n",
    "                color=colors[i], linewidth=2, label=label)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve Comparison (Micro-Average)')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Single experiment - no comparison plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CIFAR-10 ROC Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Catalog: {ml.host_name}:{ml.catalog_id}\")\n",
    "print(f\"Execution: {execution.execution_rid}\")\n",
    "print(f\"Ground truth: Execution {gt_execution} ({len(gt_lookup)} labels)\")\n",
    "print(f\"Classes: {n_classes}\")\n",
    "print(f\"Experiments analyzed: {len(experiments)}\")\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    print(f\"\\n  {exp_name}:\")\n",
    "    if 'source_execution' in exp:\n",
    "        print(f\"    Source Execution: {exp['source_execution']}\")\n",
    "    print(f\"    Samples: {exp['n_samples']}\")\n",
    "    print(f\"    Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"    Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"    Macro-AUC: {exp['roc_auc']['macro']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete execution and upload outputs\n",
    "execution.upload_execution_outputs()\n",
    "print(f\"\\nExecution completed: {execution.execution_rid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
