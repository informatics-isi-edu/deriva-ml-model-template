{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis for CIFAR-10 CNN Experiments\n",
    "\n",
    "This notebook computes ROC (Receiver Operating Characteristic) curves and AUC (Area Under Curve) scores for CIFAR-10 CNN classification experiments. It compares model performance across different training configurations.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load prediction probability files from DerivaML catalog as assets\n",
    "2. Retrieve ground truth labels from the Image_Classification feature table\n",
    "3. Compute per-class and micro/macro-averaged ROC curves\n",
    "4. Generate comparison visualizations\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Prediction probability CSV files with columns: `Image_RID`, `Predicted_Class`, `prob_<classname>` for each class\n",
    "- Ground truth labels stored in the Image_Classification feature (from a labeling execution with no confidence scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from deriva_ml.execution import run_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Initialize Notebook\n",
    "\n",
    "Initialize the notebook with DerivaML execution context. This single call:\n",
    "1. Loads all configuration modules\n",
    "2. Resolves the hydra-zen configuration\n",
    "3. Creates the DerivaML connection\n",
    "4. Creates a workflow and execution context\n",
    "5. Downloads any specified assets\n",
    "\n",
    "Override configuration at runtime:\n",
    "```python\n",
    "ml, execution, config = run_notebook(\n",
    "    \"roc_analysis\",\n",
    "    overrides=[\"assets=roc_quick_probabilities\"],  # Analyze single experiment\n",
    ")\n",
    "```\n",
    "\n",
    "Available asset configurations (see `src/configs/assets.py`):\n",
    "- `roc_quick_probabilities` - cifar10_quick experiment only\n",
    "- `roc_extended_probabilities` - cifar10_extended experiment only  \n",
    "- `roc_comparison_probabilities` - both experiments (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize notebook - this single call handles all setup\n",
    "ml, execution, config = run_notebook(\"roc_analysis\", workflow_type=\"ROC Analysis Notebook\")\n",
    "\n",
    "print(f\"Connected to {ml.host_name}, catalog {ml.catalog_id}\")\n",
    "print(f\"Execution: {execution.execution_rid}\")\n",
    "print(f\"Assets: {config.assets}\")\n",
    "print(f\"Show per-class curves: {config.show_per_class}\")\n",
    "print(f\"Confidence threshold: {config.confidence_threshold}\")\n",
    "print(f\"Downloaded: {list(execution.asset_paths.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load Probability Files from Assets\n",
    "\n",
    "Load the prediction probability CSV files from the downloaded assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probability files from downloaded assets\n",
    "experiments = []\n",
    "for asset_table, asset_list in execution.asset_paths.items():\n",
    "    for asset_path in asset_list:\n",
    "        if asset_path.file_name.name == \"prediction_probabilities.csv\":\n",
    "            df = pd.read_csv(asset_path.file_name)\n",
    "            experiments.append({\n",
    "                'asset_rid': asset_path.asset_rid,\n",
    "                'file_name': asset_path.file_name.name,\n",
    "                'data': df\n",
    "            })\n",
    "            print(f\"Loaded {len(df)} predictions from asset {asset_path.asset_rid}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(experiments)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Load configuration files from downloaded assets\n",
    "configs = {}\n",
    "for asset_table, asset_list in execution.asset_paths.items():\n",
    "    for asset_path in asset_list:\n",
    "        if asset_path.file_name.name == \"configuration.json\":\n",
    "            with open(asset_path.file_name) as f:\n",
    "                configs[asset_path.asset_rid] = json.load(f)\n",
    "\n",
    "# Match configs to experiments and extract experiment names\n",
    "asset_rids = config.assets\n",
    "for exp in experiments:\n",
    "    pred_rid = exp['asset_rid']\n",
    "    if pred_rid in asset_rids:\n",
    "        pred_idx = asset_rids.index(pred_rid)\n",
    "        if pred_idx + 1 < len(asset_rids):\n",
    "            config_rid = asset_rids[pred_idx + 1]\n",
    "            if config_rid in configs:\n",
    "                exp['config'] = configs[config_rid]\n",
    "                exp['config_rid'] = config_rid\n",
    "                \n",
    "                # Extract experiment name from model_config._name_ or use a default\n",
    "                cfg = configs[config_rid]\n",
    "                if 'model_config' in cfg and '_name_' in cfg['model_config']:\n",
    "                    exp['name'] = cfg['model_config']['_name_']\n",
    "                else:\n",
    "                    exp['name'] = f\"Experiment {pred_rid}\"\n",
    "\n",
    "# Find source execution details for each prediction asset\n",
    "pb = ml.pathBuilder()\n",
    "ml_schema = pb.schemas['deriva-ml']\n",
    "\n",
    "for exp in experiments:\n",
    "    # Get source execution RID\n",
    "    asset_exec_table = ml_schema.tables['Execution_Asset_Execution']\n",
    "    rows = list(asset_exec_table.filter(\n",
    "        (asset_exec_table.Execution_Asset == exp['asset_rid']) & \n",
    "        (asset_exec_table.Asset_Role == 'Output')\n",
    "    ).entities())\n",
    "    \n",
    "    if rows:\n",
    "        exec_rid = rows[0]['Execution']\n",
    "        exp['source_execution'] = exec_rid\n",
    "        exp['execution_url'] = f\"https://{ml.host_name}/chaise/record/#{ml.catalog_id}/deriva-ml:Execution/RID={exec_rid}\"\n",
    "        \n",
    "        # Get execution details\n",
    "        exec_table = ml_schema.tables['Execution']\n",
    "        exec_rows = list(exec_table.filter(exec_table.RID == exec_rid).entities())\n",
    "        if exec_rows:\n",
    "            exp['execution_record'] = exec_rows[0]\n",
    "        \n",
    "        # Get input datasets\n",
    "        dataset_exec_table = ml_schema.tables['Dataset_Execution']\n",
    "        dataset_links = list(dataset_exec_table.filter(dataset_exec_table.Execution == exec_rid).entities())\n",
    "        \n",
    "        exp['input_datasets'] = []\n",
    "        for link in dataset_links:\n",
    "            dataset_rid = link['Dataset']\n",
    "            dataset_table = ml_schema.tables['Dataset']\n",
    "            dataset_rows = list(dataset_table.filter(dataset_table.RID == dataset_rid).entities())\n",
    "            if dataset_rows:\n",
    "                ds = dataset_rows[0]\n",
    "                ds_info = {\n",
    "                    'rid': dataset_rid,\n",
    "                    'description': ds.get('Description', ''),\n",
    "                    'url': f\"https://{ml.host_name}/chaise/record/#{ml.catalog_id}/deriva-ml:Dataset/RID={dataset_rid}\"\n",
    "                }\n",
    "                # Get version info\n",
    "                if ds.get('Version'):\n",
    "                    version_table = ml_schema.tables['Dataset_Version']\n",
    "                    version_rows = list(version_table.filter(version_table.RID == ds['Version']).entities())\n",
    "                    if version_rows:\n",
    "                        ds_info['version'] = version_rows[0].get('Version', '')\n",
    "                exp['input_datasets'].append(ds_info)\n",
    "        \n",
    "        # Get input assets\n",
    "        input_asset_rows = list(asset_exec_table.filter(\n",
    "            (asset_exec_table.Execution == exec_rid) & \n",
    "            (asset_exec_table.Asset_Role == 'Input')\n",
    "        ).entities())\n",
    "        \n",
    "        exp['input_assets'] = []\n",
    "        asset_table = ml_schema.tables['Execution_Asset']\n",
    "        for row in input_asset_rows:\n",
    "            asset_rid = row['Execution_Asset']\n",
    "            asset_rows = list(asset_table.filter(asset_table.RID == asset_rid).entities())\n",
    "            if asset_rows:\n",
    "                asset = asset_rows[0]\n",
    "                exp['input_assets'].append({\n",
    "                    'rid': asset_rid,\n",
    "                    'filename': asset.get('Filename', ''),\n",
    "                    'url': f\"https://{ml.host_name}/chaise/record/#{ml.catalog_id}/deriva-ml:Execution_Asset/RID={asset_rid}\"\n",
    "                })\n",
    "\n",
    "# Display each experiment's information sequentially\n",
    "display(Markdown(\"## Experiment Configurations\"))\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    \n",
    "    # 1) Experiment RID with Chaise link\n",
    "    if 'execution_url' in exp:\n",
    "        display(Markdown(f\"---\\n### {exp_name}\\n\\n**Source Execution:** [{exp['source_execution']}]({exp['execution_url']})\"))\n",
    "    else:\n",
    "        display(Markdown(f\"---\\n### {exp_name}\"))\n",
    "    \n",
    "    # 2) Experiment description as markdown\n",
    "    if 'execution_record' in exp and exp['execution_record'].get('Description'):\n",
    "        display(Markdown(exp['execution_record']['Description']))\n",
    "    \n",
    "    # 3) Table of model configuration parameters\n",
    "    if 'config' in exp and 'model_config' in exp['config']:\n",
    "        mc = exp['config']['model_config']\n",
    "        config_rows = []\n",
    "        for key, val in sorted(mc.items()):\n",
    "            if not key.startswith('_'):\n",
    "                config_rows.append({'Parameter': key, 'Value': val})\n",
    "        \n",
    "        if config_rows:\n",
    "            config_df = pd.DataFrame(config_rows).set_index('Parameter')\n",
    "            display(Markdown(\"**Model Configuration:**\"))\n",
    "            display(config_df)\n",
    "    \n",
    "    # 4) List of input datasets with version and descriptions\n",
    "    if exp.get('input_datasets'):\n",
    "        display(Markdown(\"**Input Datasets:**\"))\n",
    "        for ds in exp['input_datasets']:\n",
    "            version_str = f\" (v{ds['version']})\" if ds.get('version') else \"\"\n",
    "            desc_str = f\" — {ds['description']}\" if ds.get('description') else \"\"\n",
    "            display(HTML(f\"• <a href='{ds['url']}' target='_blank'>{ds['rid']}</a>{version_str}{desc_str}\"))\n",
    "    \n",
    "    # 5) Table of input asset RIDs\n",
    "    if exp.get('input_assets'):\n",
    "        display(Markdown(\"**Input Assets:**\"))\n",
    "        asset_rows = [{'RID': a['rid'], 'Filename': a['filename']} for a in exp['input_assets']]\n",
    "        asset_df = pd.DataFrame(asset_rows)\n",
    "        # Make RIDs clickable\n",
    "        def make_link(row):\n",
    "            asset = next((a for a in exp['input_assets'] if a['rid'] == row['RID']), None)\n",
    "            if asset:\n",
    "                return f\"<a href='{asset['url']}' target='_blank'>{row['RID']}</a>\"\n",
    "            return row['RID']\n",
    "        asset_df['RID'] = asset_df.apply(make_link, axis=1)\n",
    "        display(HTML(asset_df.to_html(escape=False, index=False)))\n",
    "    elif 'input_assets' in exp:\n",
    "        display(Markdown(\"**Input Assets:** None\"))\n",
    "\n",
    "display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs\n",
    "for exp, config in configs:\n",
    "    for exp in experiments:\n",
    "        if 'execution_url' in exp:\n",
    "            display(HTML(f\"• <b>{exp.get('name', exp['asset_rid'])}</b>: <a href='{exp['execution_url']}' target='_blank'>{exp['source_execution']}</a>\"))\n",
    "else:\n",
    "    display(Markdown(\"## Experiment Configurations\\n\\n*No configuration files found in assets.*\"))\n",
    "    display(Markdown(f\"**{exp}:**\"))\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Get Ground Truth Labels\n",
    "\n",
    "Retrieve ground truth labels from the `Image_Classification` feature table. This feature stores classification labels for images, potentially from multiple sources (executions).\n",
    "\n",
    "**Identifying ground truth:**\n",
    "- Ground truth labels are manually assigned and have **no confidence score** (NULL)\n",
    "- Model predictions have confidence scores from softmax probabilities\n",
    "- We identify the ground truth execution by finding labels with zero confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels from the feature table\n",
    "all_feature_values = list(ml.list_feature_values(\"Image\", \"Image_Classification\"))\n",
    "feature_df = pd.DataFrame(all_feature_values)\n",
    "\n",
    "# Ground truth labels have no confidence score (manually labeled)\n",
    "# Group by execution to identify which has ground truth\n",
    "exec_summary = feature_df.groupby('Execution').agg({\n",
    "    'Image': 'count',\n",
    "    'Confidence': lambda x: x.notna().sum()\n",
    "}).rename(columns={'Image': 'num_images', 'Confidence': 'with_confidence'})\n",
    "\n",
    "# Find execution with no confidence scores (ground truth)\n",
    "gt_mask = exec_summary['with_confidence'] == 0\n",
    "if gt_mask.any():\n",
    "    gt_execution = exec_summary[gt_mask].index[0]\n",
    "else:\n",
    "    gt_execution = exec_summary['num_images'].idxmax()\n",
    "    \n",
    "print(f\"Ground truth execution: {gt_execution}\")\n",
    "print(f\"Total ground truth labels: {exec_summary.loc[gt_execution, 'num_images']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth as lookup dictionary\n",
    "ground_truth = feature_df[feature_df['Execution'] == gt_execution][['Image', 'Image_Class']]\n",
    "gt_lookup = dict(zip(ground_truth['Image'], ground_truth['Image_Class']))\n",
    "\n",
    "# Get class names\n",
    "class_names = sorted(ground_truth['Image_Class'].unique())\n",
    "n_classes = len(class_names)\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Merge Predictions with Ground Truth\n",
    "\n",
    "Join prediction data with ground truth labels using `Image_RID` as the key. Only images that have both predictions and ground truth labels will be included in the ROC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ground truth to each experiment's predictions\n",
    "for exp in experiments:\n",
    "    df = exp['data'].copy()\n",
    "    \n",
    "    # Debug: show sample RIDs from predictions vs ground truth\n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Prediction Image_RIDs (first 5): {df['Image_RID'].head().tolist()}\")\n",
    "    print(f\"  Ground truth Image keys (first 5): {list(gt_lookup.keys())[:5]}\")\n",
    "    \n",
    "    df['True_Class'] = df['Image_RID'].map(gt_lookup)\n",
    "    # Keep only images with ground truth\n",
    "    matched = df['True_Class'].notna().sum()\n",
    "    print(f\"  Matched: {matched} / {len(df)}\")\n",
    "    \n",
    "    df = df.dropna(subset=['True_Class'])\n",
    "    exp['data'] = df\n",
    "    exp['n_samples'] = len(df)\n",
    "    if len(df) > 0:\n",
    "        exp['accuracy'] = (df['Predicted_Class'] == df['True_Class']).mean() * 100\n",
    "        print(f\"  Accuracy: {exp['accuracy']:.1f}%\")\n",
    "    else:\n",
    "        exp['accuracy'] = float('nan')\n",
    "        print(\"  No matching samples found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Compute ROC Curves\n",
    "\n",
    "For multi-class classification, we use the **one-vs-rest (OvR)** approach:\n",
    "- Each class gets its own ROC curve treating it as positive vs. all others\n",
    "- **Micro-average**: Aggregate all classes, treating each prediction as independent\n",
    "- **Macro-average**: Simple mean of per-class AUC scores (equal weight to each class)\n",
    "\n",
    "AUC (Area Under ROC Curve) ranges from 0.5 (random) to 1.0 (perfect discrimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_metrics(df: pd.DataFrame, class_names: list[str]) -> dict:\n",
    "    \"\"\"Compute ROC curves and AUC scores for multi-class predictions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with True_Class and prob_* columns\n",
    "        class_names: Ordered list of class names\n",
    "        \n",
    "    Returns:\n",
    "        Dict with fpr, tpr, roc_auc for each class and micro/macro averages\n",
    "    \"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "    \n",
    "    # Convert labels to indices\n",
    "    y_true_idx = df['True_Class'].map(class_to_idx).values\n",
    "    y_true_bin = label_binarize(y_true_idx, classes=range(n_classes))\n",
    "    \n",
    "    # Get probability matrix\n",
    "    prob_cols = [f\"prob_{c}\" for c in class_names]\n",
    "    y_score = df[prob_cols].values\n",
    "    \n",
    "    # Compute per-class ROC\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i, name in enumerate(class_names):\n",
    "        fpr[name], tpr[name], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc[name] = auc(fpr[name], tpr[name])\n",
    "    \n",
    "    # Micro-average\n",
    "    fpr['micro'], tpr['micro'], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "    roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "    \n",
    "    # Macro-average\n",
    "    roc_auc['macro'] = np.mean([roc_auc[c] for c in class_names])\n",
    "    \n",
    "    return {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC metrics for each experiment\n",
    "for exp in experiments:\n",
    "    metrics = compute_roc_metrics(exp['data'], class_names)\n",
    "    exp.update(metrics)\n",
    "    \n",
    "    print(f\"\\nAsset {exp['asset_rid']}:\")\n",
    "    print(f\"  Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"  Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"  Macro-AUC: {exp['roc_auc']['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC comparison table\n",
    "if experiments:\n",
    "    auc_data = []\n",
    "    for exp in experiments:\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        row = {'Experiment': exp_name}\n",
    "        for c in class_names:\n",
    "            row[c] = exp['roc_auc'][c]\n",
    "        row['Micro'] = exp['roc_auc']['micro']\n",
    "        row['Macro'] = exp['roc_auc']['macro']\n",
    "        auc_data.append(row)\n",
    "\n",
    "    auc_df = pd.DataFrame(auc_data).set_index('Experiment')\n",
    "    print(\"\\nPer-class AUC scores:\")\n",
    "    display(auc_df.round(4))\n",
    "else:\n",
    "    print(\"No experiments loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(exp: dict, class_names: list[str], show_per_class: bool = True):\n",
    "    \"\"\"Plot ROC curves for an experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp: Experiment dict with fpr, tpr, roc_auc data\n",
    "        class_names: List of class names\n",
    "        show_per_class: If True, plot individual class curves. If False, only micro-average.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    fpr, tpr, roc_auc = exp['fpr'], exp['tpr'], exp['roc_auc']\n",
    "    \n",
    "    # Micro-average (always shown)\n",
    "    ax.plot(fpr['micro'], tpr['micro'], \n",
    "            label=f\"Micro-avg (AUC={roc_auc['micro']:.3f})\",\n",
    "            color='deeppink', linestyle=':', linewidth=3)\n",
    "    \n",
    "    # Per-class curves (optional based on config)\n",
    "    if show_per_class:\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))\n",
    "        for i, name in enumerate(class_names):\n",
    "            ax.plot(fpr[name], tpr[name], color=colors[i],\n",
    "                    label=f\"{name} (AUC={roc_auc[name]:.3f})\")\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    \n",
    "    # Use experiment name in title\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    ax.set_title(f\"ROC Curves: {exp_name} (Acc: {exp['accuracy']:.1f}%)\")\n",
    "    \n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for each experiment (controlled by config.show_per_class)\n",
    "for exp in experiments:\n",
    "    # Display experiment header with link to source execution\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    if 'execution_url' in exp:\n",
    "        display(HTML(f\"<h3>{exp_name}</h3><p>Source: <a href='{exp['execution_url']}' target='_blank'>Execution {exp['source_execution']}</a></p>\"))\n",
    "    else:\n",
    "        display(HTML(f\"<h3>{exp_name}</h3>\"))\n",
    "    \n",
    "    fig = plot_roc_curves(exp, class_names, show_per_class=config.show_per_class)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Experiment Comparison\n",
    "\n",
    "Compare micro-averaged ROC curves across all experiments. This visualization shows how different model configurations perform relative to each other:\n",
    "- Curves closer to the top-left corner indicate better performance\n",
    "- The diagonal dashed line represents random classification (AUC = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare micro-average ROC curves across experiments\n",
    "if len(experiments) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(experiments)))\n",
    "    \n",
    "    for i, exp in enumerate(experiments):\n",
    "        exp_name = exp.get('name', exp['asset_rid'])\n",
    "        label = f\"{exp_name} (AUC={exp['roc_auc']['micro']:.3f}, Acc={exp['accuracy']:.1f}%)\"\n",
    "        ax.plot(exp['fpr']['micro'], exp['tpr']['micro'], \n",
    "                color=colors[i], linewidth=2, label=label)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve Comparison (Micro-Average)')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Single experiment - no comparison plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CIFAR-10 ROC Analysis Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Catalog: {ml.host_name}:{ml.catalog_id}\")\n",
    "print(f\"Execution: {execution.execution_rid}\")\n",
    "print(f\"Ground truth: Execution {gt_execution} ({len(gt_lookup)} labels)\")\n",
    "print(f\"Classes: {n_classes}\")\n",
    "print(f\"Experiments analyzed: {len(experiments)}\")\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_name = exp.get('name', exp['asset_rid'])\n",
    "    print(f\"\\n  {exp_name}:\")\n",
    "    if 'source_execution' in exp:\n",
    "        print(f\"    Source Execution: {exp['source_execution']}\")\n",
    "    print(f\"    Samples: {exp['n_samples']}\")\n",
    "    print(f\"    Accuracy: {exp['accuracy']:.2f}%\")\n",
    "    print(f\"    Micro-AUC: {exp['roc_auc']['micro']:.4f}\")\n",
    "    print(f\"    Macro-AUC: {exp['roc_auc']['macro']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete execution and upload outputs\n",
    "execution.upload_execution_outputs()\n",
    "print(f\"\\nExecution completed: {execution.execution_rid}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
