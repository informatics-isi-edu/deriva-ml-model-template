{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ROC Curve Analysis for CIFAR-10 CNN Experiments\n",
    "\n",
    "This notebook computes ROC curves and AUC scores for CIFAR-10 CNN classification experiments.\n",
    "It retrieves predictions with probability scores from the DerivaML catalog and compares them\n",
    "against ground truth labels.\n",
    "\n",
    "The notebook uses:\n",
    "1. **Confidence scores** from the `Image_Classification` feature table in the catalog\n",
    "2. **Full probability distributions** from `prediction_probabilities.csv` execution asset (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from deriva_ml import DerivaML, MLVocab, RID, DerivaMLConfig\n",
    "from deriva_ml.dataset import DatasetConfigList\n",
    "from deriva_ml.execution import ExecutionConfiguration, Execution\n",
    "\n",
    "from hydra_zen import launch, zen, builds, store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Configure the execution RID to analyze. This should be the RID of a completed CIFAR-10 CNN execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell - these can be overridden by papermill\n",
    "dry_run: bool = False\n",
    "execution_rid: str = \"\"  # RID of the execution to analyze (leave empty to use latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = [f\"cfg.dry_run={dry_run}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "Initialize the Hydra configuration store with DerivaML settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration store\n",
    "import configs.deriva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class notebook_config:\n",
    "    deriva_ml: DerivaMLConfig\n",
    "    dry_run: bool\n",
    "\n",
    "notebook_defaults = [\n",
    "    \"_self_\",\n",
    "    {\"deriva_ml\": \"local\"},\n",
    "]\n",
    "\n",
    "NotebookConfig = builds(\n",
    "    notebook_config,\n",
    "    populate_full_signature=True,\n",
    "    dry_run=False,\n",
    "    hydra_defaults=notebook_defaults\n",
    ")\n",
    "\n",
    "store(NotebookConfig, name=\"roc_analysis_config\")\n",
    "store.add_to_hydra_store(overwrite_ok=True)\n",
    "\n",
    "config = launch(\n",
    "    NotebookConfig,\n",
    "    zen(NotebookConfig),\n",
    "    version_base=\"1.3\",\n",
    "    config_name=\"roc_analysis_config\",\n",
    "    job_name=\"ROCAnalysis\",\n",
    "    overrides=overrides\n",
    ").return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DerivaML connection\n",
    "ml = DerivaML.instantiate(config.deriva_ml)\n",
    "print(f\"Connected to {ml.host_name}, catalog {ml.catalog_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Find Execution to Analyze\n",
    "\n",
    "Either use the specified execution RID or find the most recent CIFAR-10 CNN execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the execution to analyze\n",
    "if execution_rid:\n",
    "    print(f\"Analyzing specified execution: {execution_rid}\")\n",
    "else:\n",
    "    # Find the most recent completed execution with predictions\n",
    "    executions = list(ml.get_table_as_dict(\"Execution\"))\n",
    "    # Filter for completed executions\n",
    "    completed = [e for e in executions if e.get(\"Status\") == \"Completed\"]\n",
    "    if completed:\n",
    "        # Sort by RID (most recent last) and take the last one\n",
    "        execution_rid = completed[-1][\"RID\"]\n",
    "        print(f\"Using most recent completed execution: {execution_rid}\")\n",
    "    else:\n",
    "        raise ValueError(\"No completed executions found\")\n",
    "\n",
    "print(f\"Execution RID: {execution_rid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Load Probability Data\n",
    "\n",
    "Try to load full probability distributions from the CSV asset. If not available, fall back to confidence scores from the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for prediction_probabilities.csv asset\n",
    "pb = ml.pathBuilder()\n",
    "exec_assets = pb.schemas[ml.ml_schema].tables[\"Execution_Asset\"]\n",
    "\n",
    "assets = list(\n",
    "    exec_assets\n",
    "    .filter(exec_assets.Execution == execution_rid)\n",
    "    .entities()\n",
    "    .fetch()\n",
    ")\n",
    "\n",
    "prob_csv_asset = None\n",
    "for asset in assets:\n",
    "    if asset.get(\"Filename\") == \"prediction_probabilities.csv\":\n",
    "        prob_csv_asset = asset\n",
    "        break\n",
    "\n",
    "if prob_csv_asset:\n",
    "    print(f\"Found prediction_probabilities.csv asset (RID: {prob_csv_asset['RID']})\")\n",
    "    use_full_probs = True\n",
    "else:\n",
    "    print(\"No prediction_probabilities.csv found, will use confidence scores from catalog\")\n",
    "    use_full_probs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions and ground truth from catalog\n",
    "prediction_table = pb.schemas[ml.ml_schema].tables[\"Execution_Image_Image_Classification\"]\n",
    "\n",
    "# Query predictions for our execution\n",
    "predictions = list(\n",
    "    prediction_table\n",
    "    .filter(prediction_table.Execution == execution_rid)\n",
    "    .entities()\n",
    "    .fetch()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(predictions)} predictions for execution {execution_rid}\")\n",
    "\n",
    "if not predictions:\n",
    "    raise ValueError(f\"No predictions found for execution {execution_rid}\")\n",
    "\n",
    "# Check if confidence scores are available\n",
    "has_confidence = any(p.get(\"Confidence\") is not None for p in predictions)\n",
    "print(f\"Confidence scores available: {has_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels from the Image_Classification feature table\n",
    "ground_truth_table = pb.schemas[ml.domain_schema].tables[\"Image_Image_Classification\"]\n",
    "\n",
    "# Get all ground truth labels\n",
    "ground_truth = list(ground_truth_table.entities().fetch())\n",
    "print(f\"Found {len(ground_truth)} ground truth labels\")\n",
    "\n",
    "# Create lookup dict: Image RID -> ground truth class\n",
    "gt_lookup = {gt[\"Image\"]: gt[\"Image_Class\"] for gt in ground_truth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build aligned arrays of predictions and ground truth\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_confidence = []  # Confidence of predicted class\n",
    "image_rids = []\n",
    "\n",
    "for pred in predictions:\n",
    "    image_rid = pred[\"Image\"]\n",
    "    if image_rid in gt_lookup:\n",
    "        y_true.append(gt_lookup[image_rid])\n",
    "        y_pred.append(pred[\"Image_Class\"])\n",
    "        y_confidence.append(pred.get(\"Confidence\", 1.0))  # Default to 1.0 if no confidence\n",
    "        image_rids.append(image_rid)\n",
    "\n",
    "print(f\"Matched {len(y_true)} predictions with ground truth\")\n",
    "\n",
    "# Get unique class names\n",
    "class_names = sorted(set(y_true))\n",
    "n_classes = len(class_names)\n",
    "print(f\"Classes ({n_classes}): {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Load Full Probability Distributions (if available)\n",
    "\n",
    "If the CSV asset exists, download and parse it for more accurate ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full probability distributions if available\n",
    "prob_matrix = None\n",
    "\n",
    "if use_full_probs and prob_csv_asset:\n",
    "    # Download the CSV file\n",
    "    import tempfile\n",
    "    import requests\n",
    "    \n",
    "    # Get the hatrac URL and download\n",
    "    url = prob_csv_asset.get(\"URL\")\n",
    "    if url:\n",
    "        # Construct full URL\n",
    "        full_url = f\"https://{ml.host_name}{url}\"\n",
    "        \n",
    "        # Download using DerivaML's session for authentication\n",
    "        response = ml.catalog.get(url)\n",
    "        \n",
    "        # Parse CSV content\n",
    "        import io\n",
    "        prob_df = pd.read_csv(io.StringIO(response.text))\n",
    "        print(f\"Loaded probability CSV with {len(prob_df)} rows\")\n",
    "        print(f\"Columns: {list(prob_df.columns)}\")\n",
    "        \n",
    "        # Extract probability columns\n",
    "        prob_cols = [f\"prob_{c}\" for c in class_names]\n",
    "        if all(col in prob_df.columns for col in prob_cols):\n",
    "            # Create RID -> probability array mapping\n",
    "            rid_to_probs = {}\n",
    "            for _, row in prob_df.iterrows():\n",
    "                rid = row[\"Image_RID\"]\n",
    "                probs = [row[col] for col in prob_cols]\n",
    "                rid_to_probs[rid] = probs\n",
    "            \n",
    "            # Build probability matrix aligned with y_true\n",
    "            prob_matrix = np.array([rid_to_probs.get(rid, [1/n_classes]*n_classes) for rid in image_rids])\n",
    "            print(f\"Built probability matrix: {prob_matrix.shape}\")\n",
    "        else:\n",
    "            print(f\"Missing probability columns, expected: {prob_cols}\")\n",
    "            use_full_probs = False\n",
    "else:\n",
    "    use_full_probs = False\n",
    "\n",
    "print(f\"\\nUsing full probability distributions: {use_full_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Compute ROC Curves\n",
    "\n",
    "For multi-class classification, we compute ROC curves using a one-vs-rest approach.\n",
    "When full probabilities are available, we get smooth ROC curves. Otherwise, we use\n",
    "confidence scores for a stepped approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class names to numeric indices\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "y_true_idx = np.array([class_to_idx[c] for c in y_true])\n",
    "y_pred_idx = np.array([class_to_idx[c] for c in y_pred])\n",
    "\n",
    "# Binarize the labels for ROC computation (one-vs-rest)\n",
    "y_true_bin = label_binarize(y_true_idx, classes=range(n_classes))\n",
    "\n",
    "# Create score matrix for ROC computation\n",
    "if use_full_probs and prob_matrix is not None:\n",
    "    # Use full probability distributions\n",
    "    y_score = prob_matrix\n",
    "    print(\"Using full probability distributions for ROC curves\")\n",
    "elif has_confidence:\n",
    "    # Use confidence scores: predicted class gets confidence, others get (1-conf)/(n-1)\n",
    "    y_score = np.zeros((len(y_pred_idx), n_classes))\n",
    "    for i, (pred_idx, conf) in enumerate(zip(y_pred_idx, y_confidence)):\n",
    "        remaining = (1 - conf) / (n_classes - 1) if n_classes > 1 else 0\n",
    "        y_score[i, :] = remaining\n",
    "        y_score[i, pred_idx] = conf\n",
    "    print(\"Using confidence scores for ROC curves\")\n",
    "else:\n",
    "    # Fall back to binary predictions (stepped ROC curves)\n",
    "    y_score = label_binarize(y_pred_idx, classes=range(n_classes)).astype(float)\n",
    "    print(\"Using binary predictions for ROC curves (no probability data available)\")\n",
    "\n",
    "print(f\"Score matrix shape: {y_score.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    fpr[class_name], tpr[class_name], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "    roc_auc[class_name] = auc(fpr[class_name], tpr[class_name])\n",
    "\n",
    "# Compute micro-average ROC curve\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average AUC\n",
    "roc_auc[\"macro\"] = np.mean([roc_auc[c] for c in class_names])\n",
    "\n",
    "print(\"\\nAUC scores per class:\")\n",
    "for class_name in class_names:\n",
    "    print(f\"  {class_name}: {roc_auc[class_name]:.4f}\")\n",
    "print(f\"\\nMicro-average AUC: {roc_auc['micro']:.4f}\")\n",
    "print(f\"Macro-average AUC: {roc_auc['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all classes\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot micro-average\n",
    "ax.plot(\n",
    "    fpr[\"micro\"], tpr[\"micro\"],\n",
    "    label=f\"Micro-average (AUC = {roc_auc['micro']:.2f})\",\n",
    "    color=\"deeppink\", linestyle=\":\", linewidth=3\n",
    ")\n",
    "\n",
    "# Plot each class\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, n_classes))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    ax.plot(\n",
    "        fpr[class_name], tpr[class_name],\n",
    "        color=colors[i],\n",
    "        label=f\"{class_name} (AUC = {roc_auc[class_name]:.2f})\"\n",
    "    )\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"Random (AUC = 0.50)\")\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(f\"ROC Curves - CIFAR-10 CNN (Execution {execution_rid})\")\n",
    "ax.legend(loc=\"lower right\", fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall accuracy\n",
    "correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
    "accuracy = correct / len(y_true) * 100\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CIFAR-10 CNN ROC Analysis Summary\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Execution RID: {execution_rid}\")\n",
    "print(f\"Total predictions: {len(y_true)}\")\n",
    "print(f\"Overall accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Data source: {'Full probabilities' if use_full_probs else ('Confidence scores' if has_confidence else 'Binary predictions')}\")\n",
    "print(f\"Micro-average AUC: {roc_auc['micro']:.4f}\")\n",
    "print(f\"Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
